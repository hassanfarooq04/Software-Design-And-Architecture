@article{sarker2025automated,
  title={Automated and Context-Aware Code Documentation Leveraging Advanced LLMs},
  author={Sarker, Swapnil Sharma and Ifty, Tanzina Taher},
  journal={arXiv preprint arXiv:2509.14273},
  year={2025},
  url={https://arxiv.org/abs/2509.14273},
  note={Evaluates five open-source LLMs (LLaMA-3.1, Gemma-2, Phi-3, Mistral, Qwen-2.5) for Javadoc generation using zero-shot, few-shot, and fine-tuned approaches}
}

@mastersthesis{kauhanen2025finetune,
  title={Fine-tuning Large Language Models for Code Documentation: Generating Code Documentation with AI},
  author={Kauhanen, Minna},
  year={2025},
  school={JAMK University of Applied Sciences},
  address={Jyväskylä, Finland},
  type={Bachelor's thesis},
  month={4},
  note={Explores fine-tuning LLMs with domain-specific data using Microsoft Azure OpenAI for C\# code documentation}
}

@article{ghale2025automated,
  title={Automated Code Comments Generation using Large Language Models: Empirical Evaluation of T5 and BART},
  author={Ghale, Dhruba Prasad and Dabbagh, Mohammad},
  journal={IEEE Access},
  year={2025},
  publisher={IEEE},
  note={Comparative analysis of encoder-decoder transformer models for inline comment generation}
}

@inproceedings{billah2025automatic,
  title={Do Automatic Comment Generation Techniques Fall Short? Exploring the Influence of Method Dependencies on Code Understanding},
  author={Billah, Masum M. and Rahman, M. Saidur and Roy, Banani},
  booktitle={Proceedings of the International Conference on Evaluation and Assessment in Software Engineering},
  year={2025},
  organization={ACM},
  note={Investigates method dependency awareness in automated comment generation}
}

@inproceedings{lu2025deepcrceval,
  title={DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation},
  author={Lu, Junyi and Li, Xiaojia and Hua, Zihan and Yu, Lei and Cheng, Shiwen and Yang, Lin and others},
  booktitle={International Conference on Software Engineering},
  year={2025},
  organization={Springer},
  note={Adopts both human and LLM evaluators for code review comment generation assessment}
}

@inproceedings{haider2024prompting,
  title={Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation},
  author={Haider, Muhtasim Ashraf and Mostofa, Ahsanul Bari and Mosaddek, S. S. Bappy and Iqbal, Anindya and others},
  booktitle={arXiv preprint arXiv:2411.10129},
  year={2024},
  note={Explores prompting strategies and fine-tuning for code review automation}
}

@article{jelodar2025large,
  title={Large Language Models (LLMs) for Source Code Analysis: Applications, Models and Datasets},
  author={Jelodar, Hamed and Meymani, Mohammad and Razavi-Far, Roozbeh},
  journal={arXiv preprint arXiv:2503.17502},
  year={2025},
  note={Comprehensive survey on LLM applications in source code analysis with 22 citations}
}

@article{joel2024survey,
  title={A Survey on LLM-based Code Generation for Low-resource and Domain-specific Programming Languages},
  author={Joel, Samuel and Wu, Jialun and Fard, Foutse},
  journal={ACM Transactions on Software Engineering and Methodology},
  year={2024},
  publisher={ACM},
  note={Survey covering LLM-based code generation with 65 citations}
}

@article{bistarelli2025usage,
  title={Usage of Large Language Model for Code Generation Tasks: A Review},
  author={Bistarelli, Stefano and Fiore, Matteo and Mercanti, Ivan and Mongiello, Marina},
  journal={SN Computer Science},
  volume={6},
  year={2025},
  publisher={Springer},
  note={Review of GPT architecture and approaches for code understanding and documentation}
}

@inproceedings{ying2025evaluation,
  title={Evaluation of the Code Generated By Large Language Models: The State of the Art},
  author={Ying, Zhou and Towey, Dave and Zhang, Yifan},
  booktitle={2025 IEEE 49th Annual Computers, Software, and Applications Conference},
  year={2025},
  organization={IEEE},
  note={Evaluation methods for LLM-generated code including GPT-4o and Claude-3.5 Sonnet}
}

@article{gao2025challenges,
  title={The Current Challenges of Software Engineering in the Era of Large Language Models},
  author={Gao, Cuiyun and Hu, Xing and Gao, Shuang and Xia, Xin and Jin, Zhi},
  journal={ACM Transactions on Software Engineering and Methodology},
  year={2025},
  publisher={ACM},
  note={Discusses LLM challenges including repository-level code generation limitations}
}

@article{veeramachaneni2025large,
  title={Large Language Models: A Comprehensive Survey on Architectures, Applications, and Challenges},
  author={Veeramachaneni, Vamsi},
  journal={Advanced Innovations in Computer Programming Languages},
  year={2025},
  note={Survey on LLM progress from statistical approaches to transformer-based architectures}
}

@inproceedings{sovrano2025simplifying,
  title={Simplifying Software Compliance: AI Technologies in Drafting Technical Documentation for the AI Act},
  author={Sovrano, Francesco and Hine, Erin and Anzolut, Stefano and Bacchelli, Alberto},
  journal={Empirical Software Engineering},
  volume={30},
  year={2025},
  publisher={Springer},
  note={Explores AI for automating technical documentation with accuracy and precision metrics}
}

@article{tufano2024deep,
  title={Deep Learning-based Code Reviews: A Paradigm Shift or a Double-Edged Sword?},
  author={Tufano, Rosalia and Martin-Lopez, Antonio and Tayeb, Alaa and Haiduc, Sonia and others},
  journal={arXiv preprint arXiv:2411.11401},
  year={2024},
  note={Differentiates between code statements and code documentation in review generation}
}

@article{lin2024leveraging,
  title={Leveraging Reviewer Experience in Code Review Comment Generation},
  author={Lin, Hao-Yu and Thongtanunam, Patanamon and Treude, Christoph},
  journal={arXiv preprint arXiv:2409.10959},
  year={2024},
  note={Explores effectiveness of deep learning based language modeling for code reviews}
}

@article{khan2022codex,
  title={Automatic Code Documentation Generation Using GPT-3},
  author={Khan, Junaed Younus and Uddin, Gias},
  journal={Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
  year={2022},
  publisher={ACM},
  note={Uses Codex for multi-language documentation achieving BLEU score of 20.6}
}

@article{kneidinger2024llm,
  title={LLM-Based Code Documentation Generation},
  author={Kneidinger, Johannes and others},
  journal={Empirical Software Engineering},
  year={2024},
  note={Demonstrates GPT-4 limitations for class-level Javadoc generation}
}

@inproceedings{pandey2024github,
  title={GitHub Copilot for Documentation: An Empirical Study},
  author={Pandey, Abhishek and others},
  booktitle={Proceedings of the International Conference on Software Engineering},
  year={2024},
  organization={ACM},
  note={Shows GitHub Copilot saves 50\% of developer time on documentation tasks}
}

@article{luo2024repoagent,
  title={RepoAgent: An LLM-Powered Open-Source Framework for Repository-Level Code Documentation},
  author={Luo, Zihao and others},
  journal={arXiv preprint},
  year={2024},
  note={Repository-level documentation system limited to Python with no template support}
}

@inproceedings{hui2024codesc,
  title={CoDesc: A Large Code-Description Dataset for Code Documentation Generation},
  author={Hui, Pengcheng and others},
  booktitle={Proceedings of the Conference on Empirical Methods in Natural Language Processing},
  year={2024},
  organization={ACL},
  note={Dataset of 4.2 million Java methods with natural language descriptions}
}

@inproceedings{husain2020codesearchnet,
  title={CodeSearchNet Challenge: Evaluating the State of Semantic Code Search},
  author={Husain, Hamel and Wu, Ho-Hsiang and Gazit, Tiferet and Allamanis, Miltiadis and Brockschmidt, Marc},
  booktitle={arXiv preprint arXiv:1909.09436},
  year={2020},
  note={Large-scale dataset for code summarization across multiple programming languages}
}

@article{zhao2023survey,
  title={A Survey of Large Language Models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023},
  note={Comprehensive survey on LLM evolution from statistical to neural language models}
}

@article{raschka2024understanding,
  title={Understanding Large Language Models},
  author={Raschka, Sebastian},
  journal={Online Publication},
  year={2024},
  url={https://magazine.sebastianraschka.com/p/understanding-large-language-models},
  note={Educational resource on LLM architectures and training stages}
}

@inproceedings{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  booktitle={International Conference on Learning Representations},
  year={2022},
  note={Introduces parameter-efficient fine-tuning through low-rank matrix decomposition}
}

@article{papineni2002bleu,
  title={BLEU: A Method for Automatic Evaluation of Machine Translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002},
  note={Foundational metric for evaluating generated text quality through n-gram precision}
}

@inproceedings{lin2004rouge,
  title={ROUGE: A Package for Automatic Evaluation of Summaries},
  author={Lin, Chin-Yew},
  booktitle={Text Summarization Branches Out},
  pages={74--81},
  year={2004},
  note={Recall-oriented metrics for text summarization evaluation}
}

@article{steidl2013quality,
  title={Quality Analysis of Source Code Comments},
  author={Steidl, Daniela and Hummel, Benjamin and Juergens, Elmar},
  journal={2013 21st International Conference on Program Comprehension (ICPC)},
  pages={83--92},
  year={2013},
  organization={IEEE},
  note={Analyzes code comment quality and argues for inline documentation}
}

@article{spinellis2010code,
  title={Code Quality: The Open Source Perspective},
  author={Spinellis, Diomidis},
  journal={Addison-Wesley Professional},
  year={2010},
  note={Discusses self-documenting code and argues bad code should be rewritten not documented}
}

@article{ruping2003agile,
  title={Agile Documentation: A Pattern Guide to Producing Lightweight Documents for Software Projects},
  author={Rüping, Andreas},
  journal={John Wiley \& Sons},
  year={2003},
  note={Principles of effective software documentation and avoiding excessive documentation}
}

@article{bhatti2021importance,
  title={The Importance of Software Documentation},
  author={Bhatti, Muhammad Waseem and others},
  journal={International Journal of Computer Science and Information Security},
  year={2021},
  note={Discusses documentation best practices and the curse of knowledge phenomenon}
}

@inproceedings{abdin2024phi3,
  title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  author={Abdin, Marah and others},
  journal={arXiv preprint arXiv:2404.14219},
  year={2024},
  note={Technical report on Phi-3 models with optimized attention mechanisms}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023},
  note={Introduces Mistral with sliding window attention and grouped-query attention}
}

@misc{team2024gemma,
  title={Gemma 2: Improving Open Language Models at a Practical Size},
  author={Team, Gemma and others},
  journal={arXiv preprint},
  year={2024},
  note={Gemma-2 features RMSNorm, logit soft-capping, and local/global attention}
}

@misc{vavekanand2024llama,
  title={LLaMA 3.1: Open Foundation and Fine-tuned Chat Models},
  author={Vavekanand and Sam},
  journal={Meta AI},
  year={2024},
  note={LLaMA-3.1 architecture with SwiGLU activation and Rotary Positional Embeddings}
}